{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f2f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & paths\n",
    "import os, re, time, json, mimetypes\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "CSV_IN = ROOT / \"map_data_demo.csv\"          # change if needed\n",
    "PUBLIC = ROOT / \"public\"\n",
    "LOGO_DIR = PUBLIC / \"logos\"\n",
    "DATA_DIR = PUBLIC / \"data\"\n",
    "\n",
    "LOGO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PLACEHOLDER_WEB = \"/logos/placeholder.png\"   # put a placeholder file here\n",
    "BRANDFETCH_KEY = \"\"  # <= add your key if you have one, else leave empty\n",
    "\n",
    "# basic knobs\n",
    "RATE_DELAY_SEC = 0.25     # be polite to free endpoints\n",
    "TIMEOUT = 6               # seconds for HTTP calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965ec23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: helpers\n",
    "\n",
    "def norm_domain(url: str) -> str:\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return \"\"\n",
    "    u = url.strip().lower()\n",
    "    u = re.sub(r\"^https?://\", \"\", u)\n",
    "    u = re.sub(r\"^www\\.\", \"\", u)\n",
    "    u = u.split(\"/\")[0]\n",
    "    return u\n",
    "\n",
    "def slugify(name: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"-\", (name or \"logo\").lower()).strip(\"-\")[:60]\n",
    "\n",
    "def pick_ext_from_headers(content_type: str, url: str) -> str:\n",
    "    if content_type:\n",
    "        ct = content_type.split(\";\")[0].strip()\n",
    "        ext = mimetypes.guess_extension(ct) or \"\"\n",
    "        if ext in (\".svg\", \".png\", \".jpg\", \".jpeg\", \".webp\", \".ico\"):\n",
    "            return ext\n",
    "    for ext in (\".svg\", \".png\", \".webp\", \".jpg\", \".jpeg\", \".ico\"):\n",
    "        if url.lower().endswith(ext):\n",
    "            return ext\n",
    "    return \".png\"\n",
    "\n",
    "def fetch_binary(url: str, headers: Optional[Dict[str, str]] = None, timeout: int = TIMEOUT):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers or {}, timeout=timeout)\n",
    "        if r.status_code == 200 and r.content and \"text/html\" not in r.headers.get(\"content-type\", \"\"):\n",
    "            return r\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def save_logo(resp, src_url: str, filename_base: str) -> str:\n",
    "    ext = pick_ext_from_headers(resp.headers.get(\"content-type\", \"\"), src_url)\n",
    "    path = LOGO_DIR / f\"{filename_base}{ext}\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "    return f\"/logos/{path.name}\"  # web path served by Vercel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1a8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: brandfetch selection & source fetchers\n",
    "\n",
    "def select_best_brandfetch_logo(payload: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Pick the best logo 'src' from Brandfetch response using type/format/size priority.\n",
    "    \"\"\"\n",
    "    logos = payload.get(\"logos\") or []\n",
    "    if not logos:\n",
    "        return None\n",
    "\n",
    "    type_rank = {\"full\": 3, \"wordmark\": 2, \"symbol\": 1, \"icon\": 1}\n",
    "    fmt_rank = {\"svg\": 5, \"png\": 4, \"webp\": 3, \"jpg\": 2, \"jpeg\": 2, \"ico\": 1}\n",
    "\n",
    "    candidates = []\n",
    "    for lg in logos:\n",
    "        ltype = lg.get(\"type\", \"\").lower()\n",
    "        for fmt in lg.get(\"formats\", []):\n",
    "            src = fmt.get(\"src\")\n",
    "            if not src:\n",
    "                continue\n",
    "            fmt_ext = fmt.get(\"format\", \"\").lower() or src.split(\"?\")[0].split(\".\")[-1].lower()\n",
    "            w = fmt.get(\"width\") or 0\n",
    "            h = fmt.get(\"height\") or 0\n",
    "            candidates.append({\n",
    "                \"src\": src,\n",
    "                \"type_score\": type_rank.get(ltype, 0),\n",
    "                \"fmt_score\": fmt_rank.get(fmt_ext, 0),\n",
    "                \"area\": (w or 0) * (h or 0)\n",
    "            })\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    candidates.sort(key=lambda c: (c[\"type_score\"], c[\"fmt_score\"], c[\"area\"]), reverse=True)\n",
    "    return candidates[0][\"src\"]\n",
    "\n",
    "def try_brandfetch(domain: str) -> Optional[str]:\n",
    "    if not BRANDFETCH_KEY:\n",
    "        return None\n",
    "    url = f\"https://api.brandfetch.io/v2/brands/{domain}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={\"Authorization\": f\"Bearer {BRANDFETCH_KEY}\"}, timeout=TIMEOUT)\n",
    "        if r.status_code == 200:\n",
    "            src = select_best_brandfetch_logo(r.json())\n",
    "            return src\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def try_clearbit(domain: str) -> Optional[str]:\n",
    "    url = f\"https://logo.clearbit.com/{domain}\"\n",
    "    r = fetch_binary(url, timeout=TIMEOUT)\n",
    "    return url if r else None\n",
    "\n",
    "def try_duckduckgo(domain: str) -> Optional[str]:\n",
    "    url = f\"https://icons.duckduckgo.com/ip3/{domain}.ico\"\n",
    "    r = fetch_binary(url, timeout=TIMEOUT)\n",
    "    return url if r else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: resolver with preference order\n",
    "\n",
    "def resolve_logo_url(domain: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (url, source_name).\n",
    "    Preference:\n",
    "      - Brandfetch (if key present) > Clearbit > DuckDuckGo\n",
    "    \"\"\"\n",
    "    if not domain:\n",
    "        return \"\", \"none\"\n",
    "\n",
    "    # Try Brandfetch first if you provided a key (best quality)\n",
    "    if BRANDFETCH_KEY:\n",
    "        bf = try_brandfetch(domain)\n",
    "        if bf: return bf, \"Brandfetch\"\n",
    "\n",
    "    # Then Clearbit (fast, good coverage)\n",
    "    cb = try_clearbit(domain)\n",
    "    if cb: return cb, \"Clearbit\"\n",
    "\n",
    "    # Then DuckDuckGo favicon\n",
    "    ddg = try_duckduckgo(domain)\n",
    "    if ddg: return ddg, \"DuckDuckGo\"\n",
    "\n",
    "    return \"\", \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7339f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved enriched CSV -> map_data_with_logos.csv\n",
      "âœ… Wrote JSON for app -> public/data/map_data.json\n",
      "ðŸ“ Logos saved in -> public/logos\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: enrich CSV, download logos, build JSON for app\n",
    "\n",
    "# --- load your data ---\n",
    "df = pd.read_csv(CSV_IN)\n",
    "\n",
    "# Column names (adjust if yours differ)\n",
    "name_col   = \"Entity\"\n",
    "domain_col = \"Domain\"\n",
    "bucket_col = \"Map Bucket 1 (Normalized)\" if \"Map Bucket 1 (Normalized)\" in df.columns else \"Map Bucket 1\"\n",
    "desc_col   = \"Description\" if \"Description\" in df.columns else None\n",
    "relevance_col = \"Relevance Flag (Y/N)\"\n",
    "\n",
    "# filter to relevance == Y\n",
    "if relevance_col in df.columns:\n",
    "    df = df[df[relevance_col].astype(str).str.upper().eq(\"Y\")].copy()\n",
    "\n",
    "df[\"__domain\"] = df[domain_col].astype(str).map(norm_domain)\n",
    "df[\"Logo_URL\"] = \"\"\n",
    "df[\"Logo_File\"] = \"\"\n",
    "df[\"Logo_Source\"] = \"\"\n",
    "\n",
    "cache: Dict[str, str] = {}   # domain -> saved /logos/.. path\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    domain = row[\"__domain\"]\n",
    "    name   = str(row.get(name_col, \"\")).strip()\n",
    "    if not domain:\n",
    "        df.at[i, \"Logo_URL\"] = \"\"\n",
    "        df.at[i, \"Logo_File\"] = PLACEHOLDER_WEB\n",
    "        df.at[i, \"Logo_Source\"] = \"placeholder\"\n",
    "        continue\n",
    "\n",
    "    # if already downloaded for same domain, reuse\n",
    "    if domain in cache:\n",
    "        df.at[i, \"Logo_URL\"] = \"\"\n",
    "        df.at[i, \"Logo_File\"] = cache[domain]\n",
    "        df.at[i, \"Logo_Source\"] = \"cached\"\n",
    "        continue\n",
    "\n",
    "    url, source = resolve_logo_url(domain)\n",
    "    if url:\n",
    "        # download binary so site works offline & on Vercel\n",
    "        resp = fetch_binary(url, timeout=TIMEOUT)\n",
    "        if resp:\n",
    "            web_path = save_logo(resp, url, slugify(domain or name))\n",
    "            df.at[i, \"Logo_URL\"] = url\n",
    "            df.at[i, \"Logo_File\"] = web_path\n",
    "            df.at[i, \"Logo_Source\"] = source\n",
    "            cache[domain] = web_path\n",
    "        else:\n",
    "            df.at[i, \"Logo_URL\"] = \"\"\n",
    "            df.at[i, \"Logo_File\"] = PLACEHOLDER_WEB\n",
    "            df.at[i, \"Logo_Source\"] = \"placeholder\"\n",
    "    else:\n",
    "        df.at[i, \"Logo_URL\"] = \"\"\n",
    "        df.at[i, \"Logo_File\"] = PLACEHOLDER_WEB\n",
    "        df.at[i, \"Logo_Source\"] = \"placeholder\"\n",
    "\n",
    "    time.sleep(RATE_DELAY_SEC)\n",
    "\n",
    "# save enriched CSV\n",
    "out_csv = ROOT / \"map_data_with_logos.csv\"\n",
    "df.drop(columns=[\"__domain\"], errors=\"ignore\").to_csv(out_csv, index=False)\n",
    "print(f\"âœ… Saved enriched CSV -> {out_csv}\")\n",
    "\n",
    "# build grouped JSON for the app: public/data/map_data.json\n",
    "grouped: Dict[str, List[Dict[str, Any]]] = {}\n",
    "for _, r in df.iterrows():\n",
    "    bucket = str(r.get(bucket_col, \"\")).strip() or \"Uncategorized\"\n",
    "    item = {\n",
    "        \"name\": str(r.get(name_col, \"\")).strip(),\n",
    "        \"description\": str(r.get(desc_col, \"\")).strip() if desc_col else \"\",\n",
    "        \"website\": str(r.get(domain_col, \"\")).strip(),\n",
    "        \"logo\": r.get(\"Logo_File\") or PLACEHOLDER_WEB\n",
    "    }\n",
    "    grouped.setdefault(bucket, []).append(item)\n",
    "\n",
    "out_json = DATA_DIR / \"map_data.json\"\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grouped, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Wrote JSON for app -> {out_json}\")\n",
    "print(f\"ðŸ“ Logos saved in -> {LOGO_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb583f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
